{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c699b918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /opt/conda/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.13)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.11.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53d7abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75470ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"gogamza/kobart-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50fe9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (..) 제거\n",
    "    sentence = re.sub(r'[#@]+[가-힣A-Za-z#]+', ' ', sentence)\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "    # 스폐셜 토큰 적용할 거면 여기 위에 영어 외 문자 공백으로 만들때 스폐셜 토큰을 넘어갈 수 있도록 지정해주면된다.\n",
    "    # 그리고 세번째 정규표현식을 지워야 할 것이다. \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8cff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_total.csv')\n",
    "val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037e679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Category = train_df['Category'].unique()\n",
    "val_Category = val_df['Category'].unique()\n",
    "def categori_ext(data, Category, tv):\n",
    "    df = pd.DataFrame()\n",
    "    for c in Category:\n",
    "        df = pd.concat([df, data[data['Category'] == c].iloc[0:int(len(data[data['Category'] == c])*0.05)]], axis = 0)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b589f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = categori_ext(train_df, train_Category, 'train')\n",
    "val_df = categori_ext(val_df, val_Category, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210693ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd321028-d5b4-55f7-9e20-2eaa262f9154</td>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6</td>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e90e721f-00d1-5114-aa5d-5f1061472a29</td>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b215f3a2-d647-59f9-8410-1274ee5edd97</td>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bda61b6-1396-5a2a-a049-0b4035e40d59</td>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  fd321028-d5b4-55f7-9e20-2eaa262f9154   \n",
       "1  c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6   \n",
       "2  e90e721f-00d1-5114-aa5d-5f1061472a29   \n",
       "3  b215f3a2-d647-59f9-8410-1274ee5edd97   \n",
       "4  0bda61b6-1396-5a2a-a049-0b4035e40d59   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary Category  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  상거래(쇼핑)  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  상거래(쇼핑)  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  상거래(쇼핑)  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  상거래(쇼핑)  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  상거래(쇼핑)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ed0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13994/13994 [00:00<00:00, 28159.20it/s]\n",
      "100%|██████████| 13994/13994 [00:00<00:00, 72843.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_text = []\n",
    "train_summary = []\n",
    "\n",
    "for tt in tqdm(train_df['Text']):\n",
    "    train_text.append(preprocess_sentence(tt))\n",
    "\n",
    "for ts in tqdm(train_df['Summary']):\n",
    "      train_summary.append(preprocess_sentence(ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b31b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1746/1746 [00:00<00:00, 24989.52it/s]\n",
      "100%|██████████| 1746/1746 [00:00<00:00, 67865.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "val_text = []\n",
    "val_summary = []\n",
    "\n",
    "for vt in tqdm(val_df['Text']):\n",
    "    val_text.append(preprocess_sentence(vt))\n",
    "\n",
    "for vs in tqdm(val_df['Summary']):\n",
    "      val_summary.append(preprocess_sentence(vs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fad25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_text[0]\n",
    "del train_summary[0]\n",
    "del val_text[0]\n",
    "del val_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e3a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_text,train_summary), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_text,val_summary), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f47bbfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나 오늘 빼빼로 25000원어치 삿는데 5000원 상품권받앗엌 개이득 머그렇게많이삿...</td>\n",
       "      <td>오늘 이마트에 가서 주임님 심부름으로 빼빼로 25000원어치를 샀는데 5000원 상...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...   \n",
       "1  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...   \n",
       "2  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...   \n",
       "3  잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...   \n",
       "4  나 오늘 빼빼로 25000원어치 삿는데 5000원 상품권받앗엌 개이득 머그렇게많이삿...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "1  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "2                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "3                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  \n",
       "4  오늘 이마트에 가서 주임님 심부름으로 빼빼로 25000원어치를 샀는데 5000원 상...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65092151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_samples = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208b4aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 13993)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 1745)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 1745)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b29c4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4324a09",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eb1760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efe354c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    input_ids = []\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i]), max_input))\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "        label_id[i].append(tokenizer.eos_token_id)  \n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_id[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "   \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'decoder_input_ids': dec_input_ids,\n",
    "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f67e67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd2f7946a1847b4b816d9f78e48921b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c868695a13343d0b219cff0c2e35de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfa8ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 32 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 5\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5\n",
    "#model.config.suppress_tokens = [23782, 14338, 22554, 234]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cee9a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 5,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f804688",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0f2208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/stopword_test\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d763e363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dce4dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b4b78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13993\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221202_014950-3kd2p05q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/3kd2p05q\" target=\"_blank\">checkpoint/stopword_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4375' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4375/4375 40:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.443900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.781800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-500\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-500/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-1000\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-1000/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-1500\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-1500/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/stopword_test/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-2000\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-2000/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/stopword_test/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-2500\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-2500/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/stopword_test/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-3000\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-3000/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/stopword_test/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-3500\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-3500/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/stopword_test/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/stopword_test/checkpoint-4000\n",
      "Configuration saved in checkpoint/stopword_test/checkpoint-4000/config.json\n",
      "Model weights saved in checkpoint/stopword_test/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/stopword_test/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/stopword_test/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/stopword_test/checkpoint-2500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4375, training_loss=3.3184619140625, metrics={'train_runtime': 2462.0701, 'train_samples_per_second': 28.417, 'train_steps_per_second': 1.777, 'total_flos': 5332526412595200.0, 'train_loss': 3.3184619140625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "245822dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1745\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 02:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'r': 0.2317488325440182, 'p': 0.23724880799787368, 'f': 0.2276783764590438}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'r': 0.08670285190083027, 'p': 0.08956570706353374, 'f': 0.08503088446710783}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'r': 0.22017302207895775, 'p': 0.2251341575506365, 'f': 0.21615822355821032}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.4915246963500977,\n",
       " 'eval_rouge-1': {'r': 0.2317488325440182,\n",
       "  'p': 0.23724880799787368,\n",
       "  'f': 0.2276783764590438},\n",
       " 'eval_rouge-2': {'r': 0.08670285190083027,\n",
       "  'p': 0.08956570706353374,\n",
       "  'f': 0.08503088446710783},\n",
       " 'eval_rouge-l': {'r': 0.22017302207895775,\n",
       "  'p': 0.2251341575506365,\n",
       "  'f': 0.21615822355821032},\n",
       " 'eval_runtime': 156.0038,\n",
       " 'eval_samples_per_second': 11.186,\n",
       " 'eval_steps_per_second': 0.179,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f06d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/f9f2ec35d3c32a1ecc7a3281f9626b7ec1913fed/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/f9f2ec35d3c32a1ecc7a3281f9626b7ec1913fed/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=5,\n",
    "                            attention_mask=attention_mask,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16149a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1745 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 32 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1745/1745 [06:05<00:00,  4.77it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_311/2811442907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummaries_after_tuning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msummaries_after_tuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msummaries_after_tuning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    }
   ],
   "source": [
    "#summaries_before_tuning = []\n",
    "#for test_sample in tqdm(test_samples):\n",
    "#    summaries_before_tuning.append(generate_summary(test_sample, model_before_tuning)[1])\n",
    "#summaries_before_tuning = list(itertools.chain(*summaries_before_tuning))    \n",
    "    \n",
    "summaries_after_tuning=[]\n",
    "for test_sample in tqdm(test_samples):\n",
    "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b26fc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9df6eff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.20882171155596074,\n",
       "  'p': 0.23552698144599316,\n",
       "  'f': 0.21425186937436974},\n",
       " 'rouge-2': {'r': 0.07532400764069774,\n",
       "  'p': 0.08611587828805094,\n",
       "  'f': 0.07730825523470342},\n",
       " 'rouge-l': {'r': 0.19856471372992923,\n",
       "  'p': 0.2239313106505082,\n",
       "  'f': 0.20373713977528604}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d72b8dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "\n",
      "Summary after \n",
      " 이력서 쓰고 영어도 해야 하고 회사 선택도 잘 해야 한다\n",
      "\n",
      "Target summary \n",
      " 이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다\n",
      "\n",
      "Text 너는 잘가라 회사 선택 잘해 알겠어 많이 힘들구나 나도 이제 이력서쓰고 영어도 해야해 우리 똥갱애지 화이팅 첫 회사 잘들어가야한다 좋은 시절 다 지났다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_100 \n",
      "\n",
      "Summary after \n",
      " 이력서를 들고 간다고 하니 그냥 가겠다고 한다\n",
      "\n",
      "Target summary \n",
      " 수업을 들으며 돈을 모으기 위해 아르바이트를 할 수 있는지 여부에 대해 이야기한다\n",
      "\n",
      "Text 너이력서들고감 아니 그냥갔어 나도 그냥 가야겟구만 이따 저나가능 언제 지금 지금은 됨 야그럼 너 차피 학원다니기 시작하면 알바 하나도 못하는겨 아니지 학원다니는게아니라 수업듣는거야 근데 지금개강하는건 수학1 이라서 이건 들을구잇는데 2월에개강하는게 미적분 인데 이건 그냥 포기할려고 일단 돈 모으는게 맞는거같음\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_200 \n",
      "\n",
      "Summary after \n",
      " 구미에 갔을 때 마스크를 쓰고 싸구려 장갑 하나 사서 끼고 다녀 마스크를 쓰고 갔다고 한다\n",
      "\n",
      "Target summary \n",
      " 방 문제로 구미에 가면서 케이에프 마스크를 썼고 내일 구리로 다시 갈 거라고 한다\n",
      "\n",
      "Text 왜 구미 갔어 때가 어느 때인데 간거야 마스크 쓰고 싸구려 장갑 하나 사서 끼고 다녀 마스크 쓰고 갔고 kf94도 2개 챙겨갔으 방때메 가야한대 낼 올라온대 kf94가 뭐야 아 한국인증필터마스크 지수 94는 초미세먼지까지 막아주는 마스크 되도록 대구는 머물지 말고 교회건물은 쳐다보지도 말아라 엉 대구 안가지 내일 구리다시갈꺼야 조심해서 잘갔다와 싸구려 면장갑 하나 사서 꼭 착용하고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_300 \n",
      "\n",
      "Summary after \n",
      " 월세가 너무 비싸서 부산이 대구보다 비싸다고 하니 무조건 전세만 생각했다고 한다\n",
      "\n",
      "Target summary \n",
      " 부산이 대구보다 월세가 비싸며 전세를 생각했지만 한 번 날리면 몇 천이 없어져서 골치가 아프다\n",
      "\n",
      "Text 아 월세는 진심 너무 비싸서 부산이 대구보다 비싸가꼬 무조건 전세만 생각햇는데 진짜 골치아프노 그치 전세는 만약에 한 번 날리면 몇천이 없어지는거\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_400 \n",
      "\n",
      "Summary after \n",
      " 엄마가 차주가 나이에 비해 차종이 좋아서 위험하다고 하니 하길 잘했다고 한다\n",
      "\n",
      "Target summary \n",
      " 나이에 비해 좋은 차를 몰고 있다고 하자 차를 몰고 오라고 한다\n",
      "\n",
      "Text 그리고 엄마가 알아서 하지 차주가 나이에 비해 차종이 좋아서 위험하데 대 아 하길 잘했네 맞아 차 좋은 거 타냐 음 제발 차 몰고 와\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_500 \n",
      "\n",
      "Summary after \n",
      " 8월 12일일에 베트남 돈을 환전했는데 위비에서 돈 입금하라는 삭제되지 않았다\n",
      "\n",
      "Target summary \n",
      " 위비에서 문자가 와서 베트남의 돈을 환전한 것을 알았다고 한다\n",
      "\n",
      "Text 8월12일 베트남 돈 환전했구나 문자온거로 알았다 위비에서 아 돈입금하라는 삭제 안하는구나 87만원\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_600 \n",
      "\n",
      "Summary after \n",
      " 욕세럼은 촉촉하고 유분은 촉촉하다며 추천한다\n",
      "\n",
      "Target summary \n",
      " 욕 세럼은 유분기가 있어서 저녁에만 쓰고 흰색은 촉촉하기만 하다\n",
      "\n",
      "Text 너도 함써봐 난추천함 촉촉해 유분마니없져 엉 완전촉촉해 욕세럼은 유분있어서 저녁에만 써 헤헤 흰거는 촉촉하기만해 와타시 엄청난 지성인거 아시죠 알져알져\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_700 \n",
      "\n",
      "Summary after \n",
      " 맥북 용량이 작은 거 아시냐고 하자 어차피 빚 있는 거 더지고 갚을까 고민이라고 한다\n",
      "\n",
      "Target summary \n",
      " 컴퓨터가 망가져서 맥북을 사고 싶은데 용량이 너무 작고 비용 때문에 고민이다\n",
      "\n",
      "Text 맥북사고천국가겠습니다 빚더미지 맥북 용량 작은거 아시져 어차피 빚있는거 빚 더지고 갚을까 고민된다 최고 512인가 너무작네 드라마 한작품 넣으면 끝이야 내맘다채울려면 일테라 그냥 외장하드사 1테라 컴터망가졌어 노트북사고싶다 내전용 아 진짜 돈들어 온김에 외장하드 사야겟다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_800 \n",
      "\n",
      "Summary after \n",
      " 목보를 보는데 김원필이 스웩 있는척 걸어나와서 하루살이를 하고 있다\n",
      "\n",
      "Target summary \n",
      " 너의 목소리가 보여를 보는데 오랜만에 김원필이 나왔고 돈을 많이 벌었을지 모르겠지만 하루살이는 아닐 것 같다\n",
      "\n",
      "Text 님님 너목보 보는데 김원필 스웩있는척 걸어나와 미친 김원필 오랜만 돈 마니 벌었을랑가 그래도 하루살이는 아닐듯 마잨\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_900 \n",
      "\n",
      "Summary after \n",
      " 전주에는 정장남이 없고 아저씨도 없어서 증권가에 사람이 있는지 물어본다\n",
      "\n",
      "Target summary \n",
      " 전주에는 정장 입은 남자과 무개념인 중장년층 남자가 없어서 정장 입은 남자를 보려면 여의도에 가야한다\n",
      "\n",
      "Text 전주엔 정장남이 없어 레알 아저씨도 없어 머얔 사람은있어 진짜 근데 정장남 없어 나도 증권가 사람볼 래 사람은 있넄 그럼 여의도가야대 너무해 여의도에 약간 교포st 들도 쫌 있자나 마저 많아\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1000 \n",
      "\n",
      "Summary after \n",
      " 거의 한을을 하다 보니 죽을 것 같고 염색을 어디서 할지 이야기하고 있다\n",
      "\n",
      "Target summary \n",
      " 염색을 한 달에 한 번 하다 보니 죽을 것 같은데 어두운 갈색은 안 좋다고 했다\n",
      "\n",
      "Text 그리고 염색할꺼얌 무슨색으루 그렇게 거의 한달을 하다보니 죽을거같아 마자 어두운갈색 어때 어디서 하게 역시 갈색 머리가 짱이지 아니 난 다 짱인디\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1100 \n",
      "\n",
      "Summary after \n",
      " 여주는 탈출 걱정은 안 해도 될 것 같고 문 다 열어놓으니까 쓰레기 버릴 걱정은 하지 않아도 될 것 같다\n",
      "\n",
      "Target summary \n",
      " 사망자가 많아서 안타깝다며 언니는 돈을 가지고 있는 게 마음이 편할 것 같다고 한다\n",
      "\n",
      "Text 넘나안타까워 사망자 넘많더라고 마죠 근데 여주는 탈출 걱정은 안해도될듯 문 다 열어놓으니까 마저마저 쓰레기버리는곳 큽 언니 살아돌아올께에에에에에에 별일없뎃지만 웅 나도 그래서 좀 고민했는데 그래도 돈 얼마 쥐고있는게 맘편할것같아소 마죠마죵 알지\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1200 \n",
      "\n",
      "Summary after \n",
      " 아직 안 안 씻고 이따가 피곤하면 그냥 잔다고 한다\n",
      "\n",
      "Target summary \n",
      " 너무 졸린데 안 씻고 있는 사람에게 피곤하면 그냥 자니까 빨리 씻으라고 하고 있다\n",
      "\n",
      "Text 오 그래도 아직까지 안졸려서 다행 나사실 아직 안씻음 게이 졸 함 아 얼른 씻어 이따 피곤하면 진짜 그냥 잔다 너몇시에잘꾸야 내일 나는 내일 공부한다 오은 포기\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1300 \n",
      "\n",
      "Summary after \n",
      " 오늘도 늦게 먹으러 늦게 와서 치킨이 먹고 싶다\n",
      "\n",
      "Target summary \n",
      " 밥을 먹었는데 치킨을 먹고 싶다고 하자 어서 오라고 한다\n",
      "\n",
      "Text 밥먹음 웅야 오늘도 늦게옴 음 르겠다웅 치킨 먹고 싶다 흑흑 얼렁와\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1400 \n",
      "\n",
      "Summary after \n",
      " 수국은 꽃 중왕인데 지금도 물시중 드느라 힘들지만 수국은 꽃 중 왕이다\n",
      "\n",
      "Target summary \n",
      " 수국은 하루 세 번 물을 줘야 한다고 알려준다\n",
      "\n",
      "Text 수국은 진짜 생긴건 수국이 꽃중왕이다 지금도 물시중드느라 너무 힘든데 수국은 진짜 물시중들다가 하루가 다 간다그래서 3번은 줘야한데 하루 와우네 그럼 너무 힘들어서 못키울듯\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1500 \n",
      "\n",
      "Summary after \n",
      " 오늘 종강해서 술을 먹고 편하게 놀다 오라고 한다\n",
      "\n",
      "Target summary \n",
      " 종강해서 술을 마시는 중인데 취한 것 같다\n",
      "\n",
      "Text 옥통왔어 와 좋겠네 맛나게 먹고 편하게 놀다와요 돈써도 괘안으니 편하게 먹고와용 오늘 종강해서 술먹자고한거 같아 집에 문제는 없는거 같고 곧들어갈께 많이 힘들었나보네 2시전에만 들어와요 나 취했나봐 몸이 되게가볍네 이미 취했고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1600 \n",
      "\n",
      "Summary after \n",
      " 뿌링클을 먹고 싶은데 한두 조각 이상 무리라고 하자 먹방을 너무 맛나게 먹었다고 한다\n",
      "\n",
      "Target summary \n",
      " 뿌링클 치킨이 먹고 싶은데 첫 입은 정말 맛있지만 한두 조각 이상은 무리라고 해도 너무 먹고 싶어서 끝나고 같이 먹으러 갈까 한다\n",
      "\n",
      "Text 뿌링클먹고싶다 뿌링뿌링 근데 그거 한두조각 이상 무리야 그렁가 먹방을너무맛나게먹어성 첫입은 진짜 짱마싯움 먹고싶다아 오또카지 같이 먹고싶따아 끈나구갈가\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1700 \n",
      "\n",
      "Summary after \n",
      " 일어나서 배민 시킬랬는데 시간이 어중간해서 저녁에 한우를 꾸어 먹기로 했다\n",
      "\n",
      "Target summary \n",
      " 일어나서 배달의 민족을 시키려고 했는데 시간이 어중간해서 저녁에 오빠가 돈을 줘서 사놓은 한우를 구워 먹기로 한다\n",
      "\n",
      "Text 일어나서 배민 시킬랬는데 시간이 너무 어중간해서 저녁에 한우 꾸어먹기로 함 저기요 한우요 와 배달에서 한우라뇨 와 나도 요새 소고기가자꾸먹고싶어요 제가 말 안하였나요 이거 또 시킴 오빠야가 돈줘서 업진살은 사로 시켜봤는데 기대됑 저기 시켜먹으세오 존맛임 살치살이 존엄 와 땟갈 미쳤다 대박임\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(summaries_after_tuning), 100):\n",
    "    print('idx_{} '.format(i))\n",
    "   # print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e9d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
