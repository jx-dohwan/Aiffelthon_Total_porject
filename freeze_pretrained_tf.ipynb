{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca18587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: rouge_score in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: datasets in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: transformers==4.24.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.24.0)\n",
      "Requirement already satisfied: transformer-utils in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: packaging in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: wandb in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.13.5)\n",
      "Requirement already satisfied: pandas in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: numpy in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (3.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: tqdm in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
      "Requirement already satisfied: rouge in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from torch==1.10.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: requests in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: nltk in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: absl-py in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: xxhash in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: dill in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: seaborn in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: colorcet in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from packaging->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (5.9.4)\n",
      "Requirement already satisfied: pathtools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (4.21.9)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: setuptools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.0.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (3.1.29)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2022.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (4.38.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (1.26.13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyct>=0.4.4 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from colorcet->transformer-utils->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from importlib-metadata->transformers==4.24.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: joblib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: param>=1.7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pyct>=0.4.4->colorcet->transformer-utils->-r requirements.txt (line 5)) (1.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9a653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback,BartTokenizerFast\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cdaee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b991fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (..) 제거\n",
    "    #sentence = re.sub(r'[#@]+[가-힣A-Za-z#]+', ' ', sentence)\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9#@]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "    # 스폐셜 토큰 적용할 거면 여기 위에 영어 외 문자 공백으로 만들때 스폐셜 토큰을 넘어갈 수 있도록 지정해주면된다.\n",
    "    # 그리고 세번째 정규표현식을 지워야 할 것이다. \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56beabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_total.csv')\n",
    "val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec707bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd321028-d5b4-55f7-9e20-2eaa262f9154</td>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6</td>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e90e721f-00d1-5114-aa5d-5f1061472a29</td>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b215f3a2-d647-59f9-8410-1274ee5edd97</td>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bda61b6-1396-5a2a-a049-0b4035e40d59</td>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  fd321028-d5b4-55f7-9e20-2eaa262f9154   \n",
       "1  c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6   \n",
       "2  e90e721f-00d1-5114-aa5d-5f1061472a29   \n",
       "3  b215f3a2-d647-59f9-8410-1274ee5edd97   \n",
       "4  0bda61b6-1396-5a2a-a049-0b4035e40d59   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary Category  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  상거래(쇼핑)  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  상거래(쇼핑)  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  상거래(쇼핑)  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  상거래(쇼핑)  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  상거래(쇼핑)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611d4a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 279992/279992 [00:09<00:00, 30882.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 279992/279992 [00:03<00:00, 86455.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_text = []\n",
    "train_summary = []\n",
    "\n",
    "for tt in tqdm(train_df['Text']):\n",
    "    train_text.append(preprocess_sentence(tt))\n",
    "\n",
    "for ts in tqdm(train_df['Summary']):\n",
    "      train_summary.append(preprocess_sentence(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd41675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 35004/35004 [00:01<00:00, 29995.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 35004/35004 [00:00<00:00, 81076.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "val_text = []\n",
    "val_summary = []\n",
    "\n",
    "for vt in tqdm(val_df['Text']):\n",
    "    val_text.append(preprocess_sentence(vt))\n",
    "\n",
    "for vs in tqdm(val_df['Summary']):\n",
    "      val_summary.append(preprocess_sentence(vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6780257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 279992/279992 [00:00<00:00, 2667074.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 35004/35004 [00:00<00:00, 2450223.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_Category = []\n",
    "val_Category= []\n",
    "\n",
    "for tc in tqdm(train_df['Category']):\n",
    "    train_Category.append(tc)\n",
    "\n",
    "for vc in tqdm(val_df['Category']):\n",
    "    val_Category.append(vc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a966afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_text)):\n",
    "    temp_cat = \"#\"+val_Category[i]+\"# \"\n",
    "    val_text[i] = temp_cat+val_text[i]\n",
    "    \n",
    "for i in range(len(train_text)):\n",
    "    temp_cat = \"#\"+train_Category[i]+\"# \"\n",
    "    train_text[i] = temp_cat+train_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "559405a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_text,train_summary), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_text,val_summary), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce12a9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  #상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우...   \n",
       "1  #상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...   \n",
       "2  #상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...   \n",
       "3  #상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...   \n",
       "4  #상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8e3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_samples = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4799f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 279992)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 35004)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 35004)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40095b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "915db260",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42410af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10acb8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    input_ids = []\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input))\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "        label_id[i].append(tokenizer.eos_token_id)  \n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_id[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "   \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'decoder_input_ids': dec_input_ids,\n",
    "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b1f54fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30031, 768, padding_idx=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special_words = [\n",
    "#                 \"#@주소#\", \"#@이모티콘#\", \"#@이름#\", \"#@URL#\", \"#@소속#\",\n",
    "#                 \"#@기타#\", \"#@전번#\", \"#@계정#\", \"#@url#\", \"#@번호#\", \"#@금융#\", \"#@신원#\",\n",
    "#                 \"#@장소#\", \"#@시스템#사진#\", \"#@시스템#동영상#\", \"#@시스템#기타#\", \"#@시스템#검색#\",\n",
    "#                 \"#@시스템#지도#\", \"#@시스템#삭제#\", \"#@시스템#파일#\", \"#@시스템#송금#\", \"#@시스템#\",\n",
    "#                 \"#개인 및 관계#\", \"#미용과 건강#\", \"#상거래(쇼핑)#\", \"#시사/교육#\", \"#식음료#\", \n",
    "#                 \"#여가 생활#\", \"#일과 직업#\", \"#주거와 생활#\", \"#행사#\",\n",
    "#                 ]\n",
    "\n",
    "\n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": special_words})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a3488d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f051074158445fa73c403a24c72615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2a4159cc8a494f9b09b7a8707b1371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74572095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 32 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5\n",
    "#model.config.suppress_tokens = [23782, 14338, 22554, 234]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "637f56a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30031\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06cd04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d371a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/freeze_pretrained_tf_test\",\n",
    "    num_train_epochs=10,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    weight_decay=0.1,\n",
    "    #label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end = True,\n",
    "    logging_strategy = 'epoch',\n",
    "    evaluation_strategy  = 'epoch',\n",
    "    save_strategy ='epoch'\n",
    "\n",
    "    # evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfd87cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4665d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97cb8e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 279992\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 175000\n",
      "  Number of trainable parameters = 123883776\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jx7789/Download/koBART/wandb/run-20221209_154357-2m6v63gm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/2m6v63gm\" target=\"_blank\">checkpoint/freeze_pretrained_tf_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140000' max='175000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140000/175000 7:27:59 < 1:51:59, 5.21 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.096800</td>\n",
       "      <td>1.856583</td>\n",
       "      <td>{'r': 0.2593372192400973, 'p': 0.26831419532059175, 'f': 0.2552187852189584}</td>\n",
       "      <td>{'r': 0.10661678669249798, 'p': 0.11225999682713486, 'f': 0.10522335067285649}</td>\n",
       "      <td>{'r': 0.24674493466549782, 'p': 0.2550383045204179, 'f': 0.24270169984873513}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.725700</td>\n",
       "      <td>1.800312</td>\n",
       "      <td>{'r': 0.2764517315021707, 'p': 0.26539558290432447, 'f': 0.262106918846797}</td>\n",
       "      <td>{'r': 0.11581564152854451, 'p': 0.11264802418406492, 'f': 0.10988731532751417}</td>\n",
       "      <td>{'r': 0.2615366340557443, 'p': 0.25090868656911214, 'f': 0.24786460058858145}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.545800</td>\n",
       "      <td>1.784661</td>\n",
       "      <td>{'r': 0.27932836236914305, 'p': 0.2696070241183919, 'f': 0.26546033966525173}</td>\n",
       "      <td>{'r': 0.11674878219703241, 'p': 0.11425068607060665, 'f': 0.11103629017107475}</td>\n",
       "      <td>{'r': 0.264616009780644, 'p': 0.255111784242901, 'f': 0.2513003640539408}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.408100</td>\n",
       "      <td>1.786517</td>\n",
       "      <td>{'r': 0.2735497703781741, 'p': 0.27660762146294376, 'f': 0.2661154080825025}</td>\n",
       "      <td>{'r': 0.11373280952619491, 'p': 0.11696176389975389, 'f': 0.1109146319366768}</td>\n",
       "      <td>{'r': 0.25917738027866366, 'p': 0.2617371330784003, 'f': 0.25196090445841846}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.293300</td>\n",
       "      <td>1.810930</td>\n",
       "      <td>{'r': 0.28038254046325617, 'p': 0.27174737998898063, 'f': 0.2671448728825775}</td>\n",
       "      <td>{'r': 0.11635771971526379, 'p': 0.11396490011492957, 'f': 0.11081786358373251}</td>\n",
       "      <td>{'r': 0.26494321886227257, 'p': 0.25644964866839565, 'f': 0.2522582965641118}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.193700</td>\n",
       "      <td>1.831726</td>\n",
       "      <td>{'r': 0.27779184038770766, 'p': 0.27238584382856545, 'f': 0.26623911658164534}</td>\n",
       "      <td>{'r': 0.11543775596302285, 'p': 0.11458789859555832, 'f': 0.11070115164748713}</td>\n",
       "      <td>{'r': 0.26217339351793373, 'p': 0.2566857110375147, 'f': 0.25108550822262066}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.107400</td>\n",
       "      <td>1.855965</td>\n",
       "      <td>{'r': 0.27943696292116865, 'p': 0.26986672477221824, 'f': 0.26569270088815505}</td>\n",
       "      <td>{'r': 0.11508534470655354, 'p': 0.1123345880961687, 'f': 0.10939412342240389}</td>\n",
       "      <td>{'r': 0.26382129505199836, 'p': 0.25437974054097207, 'f': 0.2506402723775001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>1.884968</td>\n",
       "      <td>{'r': 0.27798351651973896, 'p': 0.2688245792094734, 'f': 0.264497834886212}</td>\n",
       "      <td>{'r': 0.11472621537088412, 'p': 0.11212697965469917, 'f': 0.10913152187957287}</td>\n",
       "      <td>{'r': 0.2626177556661765, 'p': 0.25355105467719924, 'f': 0.24966485547650238}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-17500\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-17500/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-17500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-35000\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-35000/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-35000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-52500\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-52500/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-52500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-70000\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-70000/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/freeze_pretrained_tf_test/checkpoint-17500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-87500\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-87500/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-87500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-87500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-87500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/freeze_pretrained_tf_test/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-105000\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-105000/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-105000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/freeze_pretrained_tf_test/checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-122500\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-122500/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-122500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-122500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-122500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/freeze_pretrained_tf_test/checkpoint-87500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/freeze_pretrained_tf_test/checkpoint-140000\n",
      "Configuration saved in checkpoint/freeze_pretrained_tf_test/checkpoint-140000/config.json\n",
      "Model weights saved in checkpoint/freeze_pretrained_tf_test/checkpoint-140000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-140000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/freeze_pretrained_tf_test/checkpoint-140000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/freeze_pretrained_tf_test/checkpoint-105000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoint/freeze_pretrained_tf_test/checkpoint-52500 (score: 1.784661054611206).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140000, training_loss=1.4254670200892856, metrics={'train_runtime': 26883.8391, 'train_samples_per_second': 104.149, 'train_steps_per_second': 6.509, 'total_flos': 1.7072133041553408e+17, 'train_loss': 1.4254670200892856, 'epoch': 8.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db3c86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='547' max='547' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [547/547 19:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.784661054611206,\n",
       " 'eval_rouge-1': {'r': 0.27932836236914305,\n",
       "  'p': 0.2696070241183919,\n",
       "  'f': 0.26546033966525173},\n",
       " 'eval_rouge-2': {'r': 0.11674878219703241,\n",
       "  'p': 0.11425068607060665,\n",
       "  'f': 0.11103629017107475},\n",
       " 'eval_rouge-l': {'r': 0.264616009780644,\n",
       "  'p': 0.255111784242901,\n",
       "  'f': 0.2513003640539408},\n",
       " 'eval_runtime': 1199.1166,\n",
       " 'eval_samples_per_second': 29.191,\n",
       " 'eval_steps_per_second': 0.456,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d586a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30031\n",
      "}\n",
      "\n",
      "loading weights file checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at checkpoint/encoder_decoder(4)freeze_MLM_test/checkpoint-192500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=5,no_repeat_ngram_size=3,\n",
    "                            attention_mask=attention_mask, \n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94773437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 0/35004 [00:00<?, ?it/s]/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 32 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "100%|██████████████████████████████████████████████████████████████████| 35004/35004 [1:20:18<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "#summaries_before_tuning = []\n",
    "#for test_sample in tqdm(test_samples):\n",
    "#    summaries_before_tuning.append(generate_summary(test_sample, model_before_tuning)[1])\n",
    "#summaries_before_tuning = list(itertools.chain(*summaries_before_tuning))    \n",
    "    \n",
    "summaries_after_tuning=[]\n",
    "for test_sample in tqdm(test_samples):\n",
    "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n",
    "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52617f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.24189095941535405,\n",
       "  'p': 0.26588413707005826,\n",
       "  'f': 0.24452019570122735},\n",
       " 'rouge-2': {'r': 0.09658707970609752,\n",
       "  'p': 0.10892169666229526,\n",
       "  'f': 0.09821027589125457},\n",
       " 'rouge-l': {'r': 0.22957735149633765,\n",
       "  'p': 0.25212164685894584,\n",
       "  'f': 0.23195978246621796}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a6520b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "\n",
      "Summary after \n",
      " 영업팀 과장님이 보내줬는데 팀장님이 해줄지 모르겠고 저번에 부산 갈 때도 숙소로 엄청 싸웠다\n",
      "\n",
      "Target summary \n",
      " 팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다\n",
      "\n",
      "Text #일과 직업# 웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서 웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구 내말이 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고 넘 시러 일단\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1000 \n",
      "\n",
      "Summary after \n",
      " 내일 아르바이트 면접이 4시 반이라서 혼자 밥 먹고 면접 보고 오겠다고 한다\n",
      "\n",
      "Target summary \n",
      " 내일 채점 보조 아르바이트 면접이 4시 30분에 있다\n",
      "\n",
      "Text #일과 직업# 내일 못 먹겠다 나 알바 면접 네시반이얌 그래서 거기 가야되는데 집에서 가면 넘 멀엉 혼자 밥먹고 알바 면접보고 오겠움여 어디 면접 알바면접 뭐 채점보조 알써 잘갔다와\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_2000 \n",
      "\n",
      "Summary after \n",
      " 인턴인데 아직 발표를 안 했다고 하자 발표를 왜 안 하냐고 한다\n",
      "\n",
      "Target summary \n",
      " 인턴 발표가 나질 않는데 포스팅은 올라와 의문을 갖는다\n",
      "\n",
      "Text #일과 직업# #@시스템#사진# 근데아직도 발표안함 다들똑같은입장 강 안했나봄 발표를 왜 안하지 인턴인데 그니까 근데 #@기타# 포스팅은 계속 올라놈 그뭐지 담당자가다를듯 포스팅올리눈사람따로 에휴 언제까지 기다려\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_3000 \n",
      "\n",
      "Summary after \n",
      " 다이아 바로 옆 양쪽 부분이 평평한 건 없고 일자를 좋아하면 리본처럼 접히는 거 말고 일자로 해준다고 한다\n",
      "\n",
      "Target summary \n",
      " 다이아 옆을 리본처럼 접히는 것 말고 일자로 해 달라고 했다\n",
      "\n",
      "Text #주거와 생활# 다이아바로옆 양쪽부분 평평한건없어여 일자를 좋아하면 저렇게 리본처럼 접히는거말구 일자로 해준데 그얘기야 일자가조아여 알았어 리본처럼접히는게머지 넌 몰라\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_4000 \n",
      "\n",
      "Summary after \n",
      " 스를 몇 년 만에 가서 설렌다고 하였다\n",
      "\n",
      "Target summary \n",
      " 고등학교 이후로 빕스를 간 적이 없어서 오랜만에 갈 생각하니 설렌다고 얘기했다\n",
      "\n",
      "Text #주거와 생활# 빖스를몇년만에가서 설렌다 심각한애슐리덕구여꾸만 난니가부페덕구인줄알앗는데 취향이확고했었어 빕스는 고딩때이후로안간거같아 애슐리카드를보여주던#@이름#이잊지모태 내카드에 천얼마밖에없어써 그거밖에안모여써 하 실망이다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_5000 \n",
      "\n",
      "Summary after \n",
      " 교회 앞에 배추를 조그맣게 해놨는데 농약을 안 쳐서 벌레가 생겼다\n",
      "\n",
      "Target summary \n",
      " 엄마와 아빠가 심은 배추에 대해 이야기한다\n",
      "\n",
      "Text #주거와 생활# #@시스템#사진# 엄빠가 심은 배추 대박 밭두 잇냐 교회앞에 조그맣게 해놧는데 농약을 안쳐서 벌레가 엄청먹어서 막걸리뿌렷어 민간요법 저걸로 김장하면 되겠다 야 요즘 배추값 금값이여 두포기에 3마넌인가\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_6000 \n",
      "\n",
      "Summary after \n",
      " 주식에 돈을 다 물려서 생활비가 제로에 수렴했고 아르바이트비도 거의 다 거지됐다\n",
      "\n",
      "Target summary \n",
      " 주식에 돈이 다 물려서 생활비가 없다고 하니 비상금을 쓰라고 하니 그건 부모님 생신이나 어버이날에 써야 한다고 안된다고 한다\n",
      "\n",
      "Text #주거와 생활# 지금 주식에 돈 다 물려서 생활비 제로에 수렴함 흙흙 알바비도 거의다 거지돼써 신카 할부로 나가는제 이게 뭐람 울디마 바버야 집에 비상금 잇자나 앙대 그건 앵대 엄빠 생일 or 어버이날에 써야한다고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_7000 \n",
      "\n",
      "Summary after \n",
      " 토요일에 12시에 끝나는데 홍대에 오면 46분이 걸린다고 한다\n",
      "\n",
      "Target summary \n",
      " 토요일에 12시에 끝나는 데 이동에 걸리는 시간에 대해서 이야기한다\n",
      "\n",
      "Text #주거와 생활# 마쟈 나 토요일에 아마 12시에 끝나는데 언제 만날려 너 홍대오면 몇신데 46분 걸린대 고람 1시 쳐자다가 지금 일어났다 하 #@이름# 서울집에서 오는겨 아님 천안에서 오는겨 서울집이즤 홍대 가깝지 내가 퇴근할때 알려줄게 그치 뭐 2 30분이면\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_8000 \n",
      "\n",
      "Summary after \n",
      " 공항 가는 버스가 한 시간 반 정도 걸려서 5시 반에는 타려고 한다\n",
      "\n",
      "Target summary \n",
      " 공항버스를 타고 공항까지 걸리는 시간에 대해 이야기한다\n",
      "\n",
      "Text #주거와 생활# 공항가는버스 5시반에는 타야겠다 나두 더 일찍 가야되는거 아니야 공항까지 얼마나걸려 버스검색하니까 한시간반정도걸리네 흠 더일찍타야하나 한시간반씩이나걸려 #@이름#이 비행기가 8시5분이니까 두시간 전부터 탑승수속할걸 난 7시50분뱅기야 껄껄 후 몇시에타야하지 4시 30분엔 타야할듯 난\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_9000 \n",
      "\n",
      "Summary after \n",
      " 여의도 아이에프씨몰에서 만나서 맛있는 것을 먹기로 했다\n",
      "\n",
      "Target summary \n",
      " 둘이 여의도 아이에프시몰에서 만나 맛있는 것을 먹기로 했다\n",
      "\n",
      "Text #행사# 맛난거 먹자 응 연락할겡 오키 나 가는 중 어딨어 여의도 ifc몰 오키 #@시스템#사진# 오 필터 써서 바꾸시오 응\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_10000 \n",
      "\n",
      "Summary after \n",
      " 혼자 노는 게 재미없다며 내일 대전으로 오라고 하자 가면 우울하다고 한다\n",
      "\n",
      "Target summary \n",
      " 목요일 저녁에 동탄에서 보던가 금요일 오전에 데이트를 하자고 한다\n",
      "\n",
      "Text #행사# 혼자 노는거 재미써여 재미업써영 놀자아 대전으로오시길 갈까 내일 근데 가면 올때 넘 우울인디 금욜 오전에 안바쁘시면 데이트 어떠십니깡 힣 목욜 저녁때 동탄에서 봐도대궁 다 좋습니당\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_11000 \n",
      "\n",
      "Summary after \n",
      " 대형마트에서 대부분 저 브랜드의 상품을 팔아서 인터넷으로 산다\n",
      "\n",
      "Target summary \n",
      " 대형마트에서 대부분 저 브랜드의 저질의 상품을 팔아서 갈 필요가 없고 가지 말아야겠고 인터넷에서 산다\n",
      "\n",
      "Text #상거래(쇼핑)# 아 저질의 상품을 팔다니 저긴 갈필요가없군 가지말자 대형마트에서 대부분 저 브랜드 팔아 에휴 그래서 난 인터넷에서 사\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_12000 \n",
      "\n",
      "Summary after \n",
      " 산 슈펜 신발에 검정 스키니에 블라우스가 어울릴 것 같다고 한다\n",
      "\n",
      "Target summary \n",
      " 슈펜 신발과 검정 스키니에 어떤 옷이 어울릴지 이야기한다\n",
      "\n",
      "Text #상거래(쇼핑)# 내가산슈펜신발에 검정스키니에 위에를뭘사야잘어울릴까 그신발에블라우스가어울릴까 괜찮을가같은데 흰신발은 웬만하면 다 어울리잖 그렇치 잉\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_13000 \n",
      "\n",
      "Summary after \n",
      " 당첨되면 노스페이스에서 주는 경품 이벤트에 응모했다\n",
      "\n",
      "Target summary \n",
      " 경품 이벤트로 1등은 노트북 2등은 노스페이스 패딩이 준비되어 있다\n",
      "\n",
      "Text #상거래(쇼핑)# 경품 이벤트 응모도 했오 엥 경품 이벤트는 뭐야 당첨되면 뭐주는데 그거 1등 경품이 노트북이고 2등이 노스페이스 패딩인가 소소한 기대 품고있다 헐 노트북 그림의떡이네 크게주네 팔백집 더맘에들어\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_14000 \n",
      "\n",
      "Summary after \n",
      " 조식이 너무 비싸서 내일 전화해서 취소해 준다고 한다\n",
      "\n",
      "Target summary \n",
      " 조식을 취소하려고 했는데 취소가 안 된다고 했다고 해서 일단 전화를 해보기로 했다\n",
      "\n",
      "Text #상거래(쇼핑)# 잠등었능가 아직 큰일이야 #@이름# 조식 취소를 건의함 핫 넘 비싸성 그래서 내일 #@이름#가 전화해서 취소해준댕 아침 잘 안먹어서 아깝댕 취소 안된다 그랬던거 같은디 #@이모티콘#흑흑# 그래 그래도 일단 전화해보자\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_15000 \n",
      "\n",
      "Summary after \n",
      " 오늘 저녁에 유튜브를 잘 들어보자고 한다\n",
      "\n",
      "Target summary \n",
      " 새로운 것도 사야 하니 그만 사기로 하고 오늘 저녁에 유튜브를 잘 들어보기로 했다\n",
      "\n",
      "Text #여가 생활# 근니까 우리 너무 사는것 같아 새로운것도 사야지 마자 그만 사야지 오늘 저녁에 잘 들어보자 우튜브 유트브 그래 옹\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_16000 \n",
      "\n",
      "Summary after \n",
      " 반만 잘라도 케이크 같고 좋은 것 같다고 하자 여기다가 상자를 원형으로 달라서 케이크같이 좋은 거 같다고 한다\n",
      "\n",
      "Target summary \n",
      " 상자를 원형으로 잘라서 붙이고 칠하면 괜찮을 거 같고 나뭇잎 펠트지로 만든 거를 바닥에 붙이려고 한다\n",
      "\n",
      "Text #여가 생활# 나톱들었다지금 #@시스템#사진# 톱 이거 반만잘라도 케이크같고 좋은거같아여 여기다가 상자를 원형으로 달라서 붙이고 칠하면 케이크될거같아유 내생각엔 이렇게만들고싶은데 #@시스템#사진# 나뭇잎 펠트지로 만든거 바닥에붙이고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_17000 \n",
      "\n",
      "Summary after \n",
      " 선물해 준 게 우울한데 문체가 특이해서 읽어보라고 한다\n",
      "\n",
      "Target summary \n",
      " 선물해 준 거 너무 우울한데 문체가 정말 특이하다며 다음에 한번 읽어보라고 말한다\n",
      "\n",
      "Text #여가 생활# 니가선물해주는거 해준거 개 우울함 근데 문체겁나특이해 나도읽어봉래 담에함읽어보셈 작가가 진자 개성대박인듯 먼가 난해한데 이미지가 떠오름\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_18000 \n",
      "\n",
      "Summary after \n",
      " 벌써 월요일이라서 오늘부터 아홉시에 스타벅스에 다닐 예정이다\n",
      "\n",
      "Target summary \n",
      " 오늘부터 9시에 수영을 다니는데 할머니들이 텃새를 부린다\n",
      "\n",
      "Text #여가 생활# 벌써 월용일이다 하 내오늘부터 아홉시 스영 다닐라거 왴 그렇게일찍 일어나나 #@기타# 할매들이 텃세 부린다 아니 이제니더 나름 오래하지않앗나\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_19000 \n",
      "\n",
      "Summary after \n",
      " 아직 사랑니 뺀 데 안 아픈데 무사히 넘어갈 수 있을 것 같냐고 하자 나중에 욱신할 거라고 한다\n",
      "\n",
      "Target summary \n",
      " 아직 사랑니를 뺀 데가 아프지 않고 마취 풀리면 밥을 먹으라고 하였는데 이미 먹어버렸다\n",
      "\n",
      "Text #미용과 건강# 오 아직 사랑니 뺀 데 안아픈데 무사히 넘어갈수잇를거신가 나중에 욱신할걸 볼 탱탱 붓고 오 슬슬 마취풀리면 밥먹으라는데 이미먹어버림 배가 고픈건 참지모해\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_20000 \n",
      "\n",
      "Summary after \n",
      " 아저씨는 눈썹 문신한 게 일 년 만에 지워졌고 큰이모도 엄마가 살이 얇아서 오래간다\n",
      "\n",
      "Target summary \n",
      " 엄마는 리터치를 많이 해서 눈썹 문신이 오래 지속된다는 것을 알게 되었다\n",
      "\n",
      "Text #미용과 건강# 아저씨는 눈썹문신한거 금방 사라지지않았어 일년만에 지워짐 큰이모도 그렇구 엄마가 살이얇아서 오래가나 내가 리터치를 많이 했잔아 그렇구나 리터치때문에 오래가는거구만\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_21000 \n",
      "\n",
      "Summary after \n",
      " 백두산화산이 폭발할까 봐 걱정하고 있다\n",
      "\n",
      "Target summary \n",
      " 실시간 검색어에 백두산 화산이 있는데 우리가 가는 동안에는 안 일어났으면 좋겠다고 했다\n",
      "\n",
      "Text #시사/교육# 백두산화산폭발할까 징조잇대 응 그런가바 실검이야 백두산화산 화산우리사는동안 일어난다면 이렇게아둥바둥살필여없을탠대 라는생각이드네\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_22000 \n",
      "\n",
      "Summary after \n",
      " 신토익이 지금 토익시험 치는 거랑 같은 거냐고 묻자 2016년에 나온 신 토익이 온강이라고 한다\n",
      "\n",
      "Target summary \n",
      " 돈이 없어서 그냥 학교 도서관 연계해서 듣는 토익 인터넷 강의로 해결해 보려고 한다\n",
      "\n",
      "Text #시사/교육# 언니 근데 신토익이 지금 토익 셤 치는 거랑 같은 거지 2016년에 신토익 나온 거 온강 그거 있길래 웅 신토익이 지금토익이야 아 막 책들은 2019년 개정판 나오길래 학교 강의로 해결해 보려고 돈도 없는디 그 학교도서관 연계해서 듣는 인강 말하는거여 웅 마쟈마쟈 그거 550부터 시작해야 할 듯해\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_23000 \n",
      "\n",
      "Summary after \n",
      " 방금 볼일 끝나고 저녁 먹으러 가는 중인데 속은 아직 심해지지는 않았지만 괜찮다\n",
      "\n",
      "Target summary \n",
      " 속이 어떠냐고 해서 나아지지 않고 머리도 아프다고 했더니 챙겨 먹으라고 이야기한다\n",
      "\n",
      "Text #개인 및 관계# 어붕 늦는거야 응 방금 볼일 끝나고 저녁먹으러 가는중 속은어뗘 아직도 응 심해지진 않는데 나아지지도 않네 머리도 쫌 아프고 챙겨먹어 조심히 더 야좋아지네 응 너무 걱정마용\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_24000 \n",
      "\n",
      "Summary after \n",
      " 피시 카카오톡으로 설정 눌렀더니 숨긴 친구랑 차단 친구 숫자가 나왔다\n",
      "\n",
      "Target summary \n",
      " 카카오톡에 숨겨진 친구와 차단 친구가 꽤 많다\n",
      "\n",
      "Text #개인 및 관계# 아 나아까 피시톡으로 설정 눌렀다가 웅 숨긴친구랑 차단친구 숫자가 나오더라고 웅 몇 명이야 숨김육십몇명 차단 102 와 세상과 단절된 삶 아니냐 그정도면\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_25000 \n",
      "\n",
      "Summary after \n",
      " 숍에 도착해서 한복 픽업을 간다고 하니 갔다가 전해주고 엄마 픽업 가자고 한다\n",
      "\n",
      "Target summary \n",
      " 한복을 픽업하고 경희유치원 뒤 엄마 픽업을 가기로 한다\n",
      "\n",
      "Text #개인 및 관계# 저희 샵도착했어여 오키 난 한복 픽업간다 일찍가는군 갔다가 전해주고 엄마픽업갈까 #@이름#가 집이랑가까운거같던데 경희유치원뒤야 한복 주고가마\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_26000 \n",
      "\n",
      "Summary after \n",
      " 교수님이 맥북에서는 안 열린다고 공지했는데 나는 장점이 더 많아서 안 되는 거라고 한다\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target summary \n",
      " 가끔 학교 재학 증명서 발급해서 저장하는 프로그램같이 안 되는 것이 있지만 장점이 더 많아서 맥북을 사용한다고 한다\n",
      "\n",
      "Text #개인 및 관계# 막 교수님이 뭐가 안 열리지 맥북에서는 안영린다고 공지했길래 신기해써 근데 나는 장점이 더 많아성 안되는거 가끔 학교 재학증명서 발급해서 저장하는 이땈 프로그램 아아\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_27000 \n",
      "\n",
      "Summary after \n",
      " 현지가이드가 잘생겼었는데 유부남이었고 사무장님은 왜 결혼 여부를 물어보는지 모르겠다\n",
      "\n",
      "Target summary \n",
      " 현지 가이드가 잘생긴 유부남인데 사무장님이 결혼 여부를 물어서 불편했다\n",
      "\n",
      "Text #개인 및 관계# 현지가이드가 잘생겼었어 아 존잼이겠는데 근데 유부남이었어 아쉘 쉣 사무장님은 왜 결혼 여부를 물어봐서 사람 불편하게 하냐 불편한 사람은 나임 앜개웃곀\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_28000 \n",
      "\n",
      "Summary after \n",
      " 점심을 늦게 먹어서 늦은잠을 잤다\n",
      "\n",
      "Target summary \n",
      " 늦게 일어나서 점심을 늦게 먹었다고 하며 점심 겸 저녁이 아니라 늦은 점심이라고 말한다\n",
      "\n",
      "Text #개인 및 관계# 왜캐 늦게멋엇냐 점심 늦게일어났우닠가 점저를 드셧네 모래 그게어케점저녀 점심이지 늦은잠심 세시에 먹음 점저아닙니까\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_29000 \n",
      "\n",
      "Summary after \n",
      " 온라인에서 팔면 사고 싶고 수정과처럼 따뜻하게 겨율에 먹고 싶다\n",
      "\n",
      "Target summary \n",
      " 작은 것은 금방 먹었고 온라인에서 팔면 수정과처럼 겨울에 먹고 싶다\n",
      "\n",
      "Text #개인 및 관계# 작은거 순삭이다 그치 온라인에서 팔면 사고싶다 따뜻하게 겨율에 먹고싶다 수정과처럼 그치 하 이제 1장씀 아\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_30000 \n",
      "\n",
      "Summary after \n",
      " 3일까지 리그 오브 레전드를 사기로 했다고 하자 그전에 만나서 하면 된다고 한다\n",
      "\n",
      "Target summary \n",
      " 우리 서로 3일까지 리그 오브 레전드를 안 하기로 했는데 연습을 해야 야 새로운 챔피언을 할 수 있으니 너랑 만나서 해야겠다\n",
      "\n",
      "Text #개인 및 관계# 우리서로 3일까지 롤 사리기로함 악 안되는데 #@이름# 그전에 너랑 만나서 하면됨 조금 연습해야 새챔하는디 얘랑은 오늘 이루로 볼수가업더 아고 맨날 하루씩 쉬어서 피곤해해 너랑하지모 조만간 볼꾸니까\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_31000 \n",
      "\n",
      "Summary after \n",
      " 나한테 인성을 논할 때는 아닌 것 같고 국가시험을 보는데 망하라고 한다\n",
      "\n",
      "Target summary \n",
      " 국가시험 보는 사람에게 시험 망하라고 얘기했던 사람이 면접 볼 때도 욕해준다고 한다\n",
      "\n",
      "Text #개인 및 관계# 인성 구린놈아 너가 나한테 인성을 논할땐 아닌듯 난 나 국가시험 보는데 니가 나한테 시험 망해라 라고 말한거 아직도 생생하게 기억남 첨들어봄 그런소리 아 내가 그랬어 와 진짜 나느 과거에도 정말 열심히 최선을 다하며 살았구나 진짜 맘에 든다 너 이제 면접 볼 시즌 되면 그때도 취업 버전으로 욕해줌 가만안둔다 그땐 가만안두면 어쩔껀뎁 나랑 같이 재수하자 넌 시험 재수 난 변시재수\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_32000 \n",
      "\n",
      "Summary after \n",
      " 배가 고파서 샌드위치밖에 안 먹었는데 오늘 돈코츠 라멘이 너무 맛이 없었다\n",
      "\n",
      "Target summary \n",
      " 오늘 음식을 조금밖에 안 먹어서 배가 고프다고 서로에게 말하고 있다\n",
      "\n",
      "Text #식음료# 나 배가 너무 고프다 나도 샌드위치밖에 안먹엇어 오늘 돈코츠라멘이 너무맛없엇어 후 나도 라면3입이링 요거트하나끝 다 버렷어 바렷어\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_33000 \n",
      "\n",
      "Summary after \n",
      " 서울에서 먹고 싶은 게 있냐며 이에 대해 이야기하고 있다\n",
      "\n",
      "Target summary \n",
      " 서울에서 고르곤졸라를 사다 달라고 하고 있다\n",
      "\n",
      "Text #식음료# 아니 뭐 좀 사갈까 서울에서 먹고싶은거 있어 고르곤졸라 아니 그건 서울아니어도 되자나 그럼 서울에 머잇는지 몰라 보다 생각나면 말해 인스타 같은데서 고르곤졸라면 됨 몇일전에 먹엇는데 또 먹고싶가 되게좋아하네 서울오면 내가 맛난 곳으로 모시죠 서울언제 갈줄알고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_34000 \n",
      "\n",
      "Summary after \n",
      " 씨그램이라는 탄산수에 코카콜라 맛을 제일 좋아한다\n",
      "\n",
      "Target summary \n",
      " 씨그램 탄산수 중에 라임 맛을 좋아하고 코카콜라 회사인데 엘지가 코카콜라를 인수했다고 한다\n",
      "\n",
      "Text #식음료# 씨그램이라는 탄산수에요 3가지맛이 있는데 저는 라임맛을 제일 좋아하죠 그거 코카콜라꺼 였네요 네 맞아요 엘지가 인수했잖아요 코카콜라 그건 몰랐던 사실인데요 정말이에요 네 꽤 오래되었는데 잘모르시더라고요 2007년도에 인수했어요\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_35000 \n",
      "\n",
      "Summary after \n",
      " 닭갈비가 맛있었다고 하자 먹기 싫다고 했는데 역시 너의 선택이었다고 한다\n",
      "\n",
      "Target summary \n",
      " 아깐 먹기 싫다고 했지만 닭갈비 맛있었으며 역시 너의 선택은 최고이고 볶음밥도 최고라고 한다\n",
      "\n",
      "Text #식음료# 닭갈비 맛있었다 맛나맛나 그티웅 아깐 먹기 싫다더니 역시 너의 선택 최고엿어 요즘 내 메뉴선택 죽이지 응응 볶음밥도 채고 맛있었어\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(summaries_after_tuning), 1000):\n",
    "    print('idx_{} '.format(i))\n",
    "    #print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fac70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
