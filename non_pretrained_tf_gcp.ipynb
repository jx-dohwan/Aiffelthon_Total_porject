{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176e7119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: rouge_score in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: datasets in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (2.7.1)\n",
      "Requirement already satisfied: transformers==4.24.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.24.0)\n",
      "Requirement already satisfied: transformer-utils in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: packaging in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: wandb in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.13.5)\n",
      "Requirement already satisfied: pandas in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: numpy in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (3.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: tqdm in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
      "Requirement already satisfied: rouge in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from torch==1.10.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: filelock in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2022.10.31)\n",
      "Requirement already satisfied: importlib-metadata in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: requests in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: absl-py in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: nltk in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (2022.11.0)\n",
      "Requirement already satisfied: multiprocess in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: xxhash in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: seaborn in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: colorcet in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from packaging->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: pathtools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (4.21.9)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (3.1.29)\n",
      "Requirement already satisfied: setproctitle in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2022.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2022.9.24)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from colorcet->transformer-utils->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from importlib-metadata->transformers==4.24.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: joblib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: param>=1.7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pyct>=0.4.4->colorcet->transformer-utils->-r requirements.txt (line 5)) (1.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc7f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback,BartTokenizerFast\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3362ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"checkpoint/non_MLM_test/checkpoint-35000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (..) 제거\n",
    "    #sentence = re.sub(r'[#@]+[가-힣A-Za-z#]+', ' ', sentence)\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9#@]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "    # 스폐셜 토큰 적용할 거면 여기 위에 영어 외 문자 공백으로 만들때 스폐셜 토큰을 넘어갈 수 있도록 지정해주면된다.\n",
    "    # 그리고 세번째 정규표현식을 지워야 할 것이다. \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d26aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_total.csv')\n",
    "val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed4e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Category = train_df['Category'].unique()\n",
    "val_Category = val_df['Category'].unique()\n",
    "def categori_ext(data, Category, tv):\n",
    "    df = pd.DataFrame()\n",
    "    for c in Category:\n",
    "        df = pd.concat([df, data[data['Category'] == c].iloc[0:int(len(data[data['Category'] == c])*0.05)]], axis = 0)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac67a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = categori_ext(train_df, train_Category, 'train')\n",
    "val_df = categori_ext(val_df, val_Category, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1293a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd321028-d5b4-55f7-9e20-2eaa262f9154</td>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6</td>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e90e721f-00d1-5114-aa5d-5f1061472a29</td>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b215f3a2-d647-59f9-8410-1274ee5edd97</td>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bda61b6-1396-5a2a-a049-0b4035e40d59</td>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  fd321028-d5b4-55f7-9e20-2eaa262f9154   \n",
       "1  c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6   \n",
       "2  e90e721f-00d1-5114-aa5d-5f1061472a29   \n",
       "3  b215f3a2-d647-59f9-8410-1274ee5edd97   \n",
       "4  0bda61b6-1396-5a2a-a049-0b4035e40d59   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary Category  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  상거래(쇼핑)  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  상거래(쇼핑)  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  상거래(쇼핑)  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  상거래(쇼핑)  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  상거래(쇼핑)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebdf18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 13994/13994 [00:00<00:00, 30935.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 13994/13994 [00:00<00:00, 81889.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_text = []\n",
    "train_summary = []\n",
    "\n",
    "for tt in tqdm(train_df['Text']):\n",
    "    train_text.append(preprocess_sentence(tt))\n",
    "\n",
    "for ts in tqdm(train_df['Summary']):\n",
    "      train_summary.append(preprocess_sentence(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0eabb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1746/1746 [00:00<00:00, 30027.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1746/1746 [00:00<00:00, 84015.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "val_text = []\n",
    "val_summary = []\n",
    "\n",
    "for vt in tqdm(val_df['Text']):\n",
    "    val_text.append(preprocess_sentence(vt))\n",
    "\n",
    "for vs in tqdm(val_df['Summary']):\n",
    "      val_summary.append(preprocess_sentence(vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535d8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_text,train_summary), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_text,val_summary), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6d74d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...   \n",
       "1  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...   \n",
       "2  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...   \n",
       "3  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...   \n",
       "4  잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65a4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_samples = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a7c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Text', 'Summary'],\n",
      "    num_rows: 13994\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Text', 'Summary'],\n",
      "    num_rows: 1746\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Text', 'Summary'],\n",
      "    num_rows: 1746\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06c6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee34e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d240c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bdc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    input_ids = []\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input))\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "        label_id[i].append(tokenizer.eos_token_id)  \n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_id[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "   \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'decoder_input_ids': dec_input_ids,\n",
    "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8f62ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30022, 768, padding_idx=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_words = [\n",
    "                \"#@주소#\", \"#@이모티콘#\", \"#@이름#\", \"#@URL#\", \"#@소속#\",\n",
    "                \"#@기타#\", \"#@전번#\", \"#@계정#\", \"#@url#\", \"#@번호#\", \"#@금융#\", \"#@신원#\",\n",
    "                \"#@장소#\", \"#@시스템#사진#\", \"#@시스템#동영상#\", \"#@시스템#기타#\", \"#@시스템#검색#\",\n",
    "                \"#@시스템#지도#\", \"#@시스템#삭제#\", \"#@시스템#파일#\", \"#@시스템#송금#\", \"#@시스템#\",\n",
    "                ]\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_words})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16ab9200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b657afb1f1146978305df73e5cb58cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1095311ccea41a5b72445e8ff41a3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a78dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 32 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5\n",
    "#model.config.suppress_tokens = [23782, 14338, 22554, 234]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa97813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"checkpoint/non_MLM_test/checkpoint-35000\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30022\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca6db762",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f47a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/non_pretrained_tf_test\",\n",
    "    num_train_epochs=40,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end = True,\n",
    "    logging_strategy = 'epoch',\n",
    "    evaluation_strategy  = 'epoch',\n",
    "    save_strategy ='epoch'\n",
    "\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a9f3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f169c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fc27b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 13994\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 35000\n",
      "  Number of trainable parameters = 123876864\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jx7789/Download/koBART/wandb/run-20221206_153231-vooaruo4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/vooaruo4\" target=\"_blank\">checkpoint/non_pretrained_tf_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23626' max='35000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23626/35000 3:09:06 < 1:31:02, 2.08 it/s, Epoch 27/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.075700</td>\n",
       "      <td>5.830609</td>\n",
       "      <td>{'r': 0.062384370600150924, 'p': 0.0991577294412344, 'f': 0.07417142944636296}</td>\n",
       "      <td>{'r': 0.007249671084145454, 'p': 0.011301176550317439, 'f': 0.008453589018988187}</td>\n",
       "      <td>{'r': 0.06100202376315585, 'p': 0.09702996348185328, 'f': 0.07252134111229286}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.504300</td>\n",
       "      <td>5.618076</td>\n",
       "      <td>{'r': 0.0724450742309209, 'p': 0.10802280718517804, 'f': 0.0840226415993384}</td>\n",
       "      <td>{'r': 0.009470138478947638, 'p': 0.013591375532956283, 'f': 0.010665871212622925}</td>\n",
       "      <td>{'r': 0.07047055441664174, 'p': 0.10521640682723124, 'f': 0.08175894805913134}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.162500</td>\n",
       "      <td>5.492140</td>\n",
       "      <td>{'r': 0.08489473360554184, 'p': 0.11766666418728267, 'f': 0.0955768410132697}</td>\n",
       "      <td>{'r': 0.011397891767706538, 'p': 0.014339404073081034, 'f': 0.012084286646342058}</td>\n",
       "      <td>{'r': 0.08170766558020134, 'p': 0.11343627099640832, 'f': 0.09202252816028471}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.874500</td>\n",
       "      <td>5.415494</td>\n",
       "      <td>{'r': 0.09333291908660052, 'p': 0.1266750415805398, 'f': 0.10434839484943077}</td>\n",
       "      <td>{'r': 0.012077072341267536, 'p': 0.015319789031129223, 'f': 0.013095664716757558}</td>\n",
       "      <td>{'r': 0.09017737294061892, 'p': 0.12237441984005551, 'f': 0.10077322942814781}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.614400</td>\n",
       "      <td>5.370637</td>\n",
       "      <td>{'r': 0.10046837378144792, 'p': 0.12926882125765282, 'f': 0.10962194679067926}</td>\n",
       "      <td>{'r': 0.013495981502242597, 'p': 0.016294673253436132, 'f': 0.014232818923708526}</td>\n",
       "      <td>{'r': 0.09622368031029609, 'p': 0.12376574996849894, 'f': 0.10495306012456075}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.369600</td>\n",
       "      <td>5.358893</td>\n",
       "      <td>{'r': 0.09810639160626616, 'p': 0.12732789674370423, 'f': 0.1074560787270465}</td>\n",
       "      <td>{'r': 0.013638337477014293, 'p': 0.016453400710273555, 'f': 0.014383575763517797}</td>\n",
       "      <td>{'r': 0.09347229840899542, 'p': 0.12150781953187446, 'f': 0.10243667263120589}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.129500</td>\n",
       "      <td>5.353351</td>\n",
       "      <td>{'r': 0.1086132746116331, 'p': 0.13675350518133988, 'f': 0.11700822771286673}</td>\n",
       "      <td>{'r': 0.014825563540494429, 'p': 0.017259504050741156, 'f': 0.01529825715778673}</td>\n",
       "      <td>{'r': 0.1024587451746517, 'p': 0.12905292890258485, 'f': 0.11038139238863567}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.906000</td>\n",
       "      <td>5.365208</td>\n",
       "      <td>{'r': 0.11481813613357064, 'p': 0.14079856830387455, 'f': 0.12243298295174795}</td>\n",
       "      <td>{'r': 0.017712337004731116, 'p': 0.020324856987228128, 'f': 0.018104007570543257}</td>\n",
       "      <td>{'r': 0.10893969162465626, 'p': 0.1338001763638514, 'f': 0.1162465531112571}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.688000</td>\n",
       "      <td>5.386684</td>\n",
       "      <td>{'r': 0.11400505553597559, 'p': 0.135535755113277, 'f': 0.11978564469939458}</td>\n",
       "      <td>{'r': 0.016083536155775572, 'p': 0.01805929529297226, 'f': 0.01626233903140971}</td>\n",
       "      <td>{'r': 0.10744985129969405, 'p': 0.12778582558499688, 'f': 0.11292264307260592}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.486500</td>\n",
       "      <td>5.416964</td>\n",
       "      <td>{'r': 0.11740287493278373, 'p': 0.1402789586638385, 'f': 0.1238253937600851}</td>\n",
       "      <td>{'r': 0.01662549593027128, 'p': 0.019357956857956857, 'f': 0.0171483369922947}</td>\n",
       "      <td>{'r': 0.10941999786696954, 'p': 0.13094394786834648, 'f': 0.11547686427477519}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.296800</td>\n",
       "      <td>5.449509</td>\n",
       "      <td>{'r': 0.1179172275502936, 'p': 0.1385694894071183, 'f': 0.12332129283461975}</td>\n",
       "      <td>{'r': 0.01582668492990273, 'p': 0.018370846954823952, 'f': 0.016224301979509935}</td>\n",
       "      <td>{'r': 0.1098409469997995, 'p': 0.1292865377272593, 'f': 0.11491484600069482}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.121000</td>\n",
       "      <td>5.498324</td>\n",
       "      <td>{'r': 0.11552125734167638, 'p': 0.13879292589595502, 'f': 0.1221902032081985}</td>\n",
       "      <td>{'r': 0.01665843803290208, 'p': 0.019484035260010586, 'f': 0.017221488547093013}</td>\n",
       "      <td>{'r': 0.10863963076177434, 'p': 0.1302672517370688, 'f': 0.1147395506681104}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.957400</td>\n",
       "      <td>5.544695</td>\n",
       "      <td>{'r': 0.11403058351576226, 'p': 0.13392464835265114, 'f': 0.11917204358847573}</td>\n",
       "      <td>{'r': 0.015932322647383874, 'p': 0.017856598738951672, 'f': 0.01609307134295713}</td>\n",
       "      <td>{'r': 0.10685734409344563, 'p': 0.12519059680570385, 'f': 0.1115191097351655}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.806700</td>\n",
       "      <td>5.586387</td>\n",
       "      <td>{'r': 0.11406290638010445, 'p': 0.13572482856518647, 'f': 0.11969527126610366}</td>\n",
       "      <td>{'r': 0.017570574324468236, 'p': 0.02036911884932483, 'f': 0.017896035007421533}</td>\n",
       "      <td>{'r': 0.1073809496709515, 'p': 0.1278571839277824, 'f': 0.11267263472898152}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.670100</td>\n",
       "      <td>5.621566</td>\n",
       "      <td>{'r': 0.11510707061348226, 'p': 0.13133815308365543, 'f': 0.11868015132297637}</td>\n",
       "      <td>{'r': 0.016772849749707067, 'p': 0.01842404857842177, 'f': 0.016839989983717562}</td>\n",
       "      <td>{'r': 0.10815863826196134, 'p': 0.12327475711218389, 'f': 0.1114175918862788}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.547200</td>\n",
       "      <td>5.678284</td>\n",
       "      <td>{'r': 0.11683682615337058, 'p': 0.13631289472455843, 'f': 0.12180657964695259}</td>\n",
       "      <td>{'r': 0.01622983575974755, 'p': 0.018016358477749215, 'f': 0.01639224734960495}</td>\n",
       "      <td>{'r': 0.10951861115047418, 'p': 0.12725741277641425, 'f': 0.11394296249478558}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.434600</td>\n",
       "      <td>5.718110</td>\n",
       "      <td>{'r': 0.11832360308664823, 'p': 0.13320441239102757, 'f': 0.12121247812723776}</td>\n",
       "      <td>{'r': 0.01693187546775047, 'p': 0.01807872123254652, 'f': 0.016754340412125278}</td>\n",
       "      <td>{'r': 0.1103711909021389, 'p': 0.1240637164511817, 'f': 0.1129514733460532}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.328900</td>\n",
       "      <td>5.763917</td>\n",
       "      <td>{'r': 0.12243936659520754, 'p': 0.14104023283576567, 'f': 0.12692215212573038}</td>\n",
       "      <td>{'r': 0.01899291529762682, 'p': 0.020620885564285624, 'f': 0.018933949233803805}</td>\n",
       "      <td>{'r': 0.11512427133120909, 'p': 0.13230731022827258, 'f': 0.11916996895336758}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.238800</td>\n",
       "      <td>5.796244</td>\n",
       "      <td>{'r': 0.12109178782981181, 'p': 0.13352617975874498, 'f': 0.12277868152136619}</td>\n",
       "      <td>{'r': 0.017172876667998632, 'p': 0.01791481298126738, 'f': 0.016734854684256457}</td>\n",
       "      <td>{'r': 0.11287103098548719, 'p': 0.12435673015190912, 'f': 0.11433938919080393}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.154900</td>\n",
       "      <td>5.844096</td>\n",
       "      <td>{'r': 0.12118243935235191, 'p': 0.1354608413040291, 'f': 0.12369660291522934}</td>\n",
       "      <td>{'r': 0.01633350823775908, 'p': 0.01742480926077044, 'f': 0.01615678586842822}</td>\n",
       "      <td>{'r': 0.11364964596502675, 'p': 0.12675598556561782, 'f': 0.11583396478799941}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.080600</td>\n",
       "      <td>5.868381</td>\n",
       "      <td>{'r': 0.123456136536817, 'p': 0.13498100736804705, 'f': 0.12441310940060016}</td>\n",
       "      <td>{'r': 0.01695858096620653, 'p': 0.017968384782868302, 'f': 0.01665970015607671}</td>\n",
       "      <td>{'r': 0.1144320878778354, 'p': 0.12498562911918187, 'f': 0.11524078335555465}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.014600</td>\n",
       "      <td>5.896746</td>\n",
       "      <td>{'r': 0.12153447581443458, 'p': 0.13047889323888606, 'f': 0.12135185959040569}</td>\n",
       "      <td>{'r': 0.01642986788635932, 'p': 0.017227005130359626, 'f': 0.016094883761279877}</td>\n",
       "      <td>{'r': 0.11328731124326183, 'p': 0.12137009102694057, 'f': 0.1129645120038714}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.956100</td>\n",
       "      <td>5.921222</td>\n",
       "      <td>{'r': 0.12416430376060779, 'p': 0.13069562876064442, 'f': 0.12276792437480318}</td>\n",
       "      <td>{'r': 0.01691066539818574, 'p': 0.01773815759764181, 'f': 0.016472089020078334}</td>\n",
       "      <td>{'r': 0.11570132343802082, 'p': 0.121807764618333, 'f': 0.11436115984405168}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.903900</td>\n",
       "      <td>5.965181</td>\n",
       "      <td>{'r': 0.12303567634435275, 'p': 0.13478661957950333, 'f': 0.12408202633408869}</td>\n",
       "      <td>{'r': 0.017309749060185255, 'p': 0.01824005556318068, 'f': 0.01697709163624681}</td>\n",
       "      <td>{'r': 0.11377458537551477, 'p': 0.12442250664312436, 'f': 0.11464646073538073}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.858200</td>\n",
       "      <td>5.973176</td>\n",
       "      <td>{'r': 0.12313392648051331, 'p': 0.1323608647340204, 'f': 0.12319623146815258}</td>\n",
       "      <td>{'r': 0.017227671916252445, 'p': 0.01766009572415369, 'f': 0.016674151034498125}</td>\n",
       "      <td>{'r': 0.11491209988890108, 'p': 0.12324948618788335, 'f': 0.11478389356891548}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.816500</td>\n",
       "      <td>6.003945</td>\n",
       "      <td>{'r': 0.12332761772186694, 'p': 0.13734056882336568, 'f': 0.12548793941489}</td>\n",
       "      <td>{'r': 0.018109361677832194, 'p': 0.019038386988659884, 'f': 0.01774208095308801}</td>\n",
       "      <td>{'r': 0.11446410517390446, 'p': 0.12727053466044683, 'f': 0.11631318230560293}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/28 00:55 < 05:20, 0.07 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-875\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-875/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-875/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-1750\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-1750/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-1750/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-2625\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-2625/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-2625/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-3500\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-3500/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-4375\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-4375/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-4375/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-4375/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-4375/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-1750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-5250\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-5250/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-5250/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-2625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-6125\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-6125/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-6125/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-6125/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-6125/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-3500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-7000\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-7000/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-4375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-7875\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-7875/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-7875/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-7875/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-7875/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-5250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-8750\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-8750/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-8750/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-8750/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-8750/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-9625\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-9625/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-9625/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-9625/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-9625/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-7875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-10500\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-10500/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-8750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-11375\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-11375/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-11375/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-11375/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-11375/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-9625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-12250\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-12250/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-12250/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-12250/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-12250/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-10500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-13125\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-13125/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-13125/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-13125/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-13125/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-11375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-14000\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-14000/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-12250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-14875\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-14875/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-14875/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-14875/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-14875/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-13125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-15750\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-15750/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-15750/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-15750/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-15750/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-16625\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-16625/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-16625/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-16625/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-16625/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-14875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-17500\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-17500/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-15750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-18375\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-18375/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-18375/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-18375/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-18375/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-16625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-19250\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-19250/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-19250/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-19250/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-19250/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-17500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-20125\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-20125/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-20125/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-20125/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-20125/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-18375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-21000\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-21000/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-19250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-21875\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-21875/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-21875/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-21875/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-21875/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-20125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to checkpoint/non_pretrained_tf_test/checkpoint-22750\n",
      "Configuration saved in checkpoint/non_pretrained_tf_test/checkpoint-22750/config.json\n",
      "Model weights saved in checkpoint/non_pretrained_tf_test/checkpoint-22750/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/non_pretrained_tf_test/checkpoint-22750/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/non_pretrained_tf_test/checkpoint-22750/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/non_pretrained_tf_test/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ffc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a358bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=5,no_repeat_ngram_size=3,\n",
    "                            attention_mask=attention_mask, \n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summaries_before_tuning = []\n",
    "#for test_sample in tqdm(test_samples):\n",
    "#    summaries_before_tuning.append(generate_summary(test_sample, model_before_tuning)[1])\n",
    "#summaries_before_tuning = list(itertools.chain(*summaries_before_tuning))    \n",
    "    \n",
    "summaries_after_tuning=[]\n",
    "for test_sample in tqdm(test_samples):\n",
    "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n",
    "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9bc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(summaries_after_tuning), 100):\n",
    "    print('idx_{} '.format(i))\n",
    "    #print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7539c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
