{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0242ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /opt/conda/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.11.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90e4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e5758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"/aiffel/aiffel/Korean_Conversation_Summary/checkpoint/total_MLM_test/checkpoint-8500\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc650da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (..) 제거\n",
    "    #sentence = re.sub(r'[#@]+[가-힣A-Za-z#]+', ' ', sentence)\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9#@]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "    # 스폐셜 토큰 적용할 거면 여기 위에 영어 외 문자 공백으로 만들때 스폐셜 토큰을 넘어갈 수 있도록 지정해주면된다.\n",
    "    # 그리고 세번째 정규표현식을 지워야 할 것이다. \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98844f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_total.csv')\n",
    "val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a013c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Category = train_df['Category'].unique()\n",
    "val_Category = val_df['Category'].unique()\n",
    "def categori_ext(data, Category, tv):\n",
    "    df = pd.DataFrame()\n",
    "    for c in Category:\n",
    "        df = pd.concat([df, data[data['Category'] == c].iloc[0:int(len(data[data['Category'] == c])*0.05)]], axis = 0)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fb32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = categori_ext(train_df, train_Category, 'train')\n",
    "val_df = categori_ext(val_df, val_Category, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052363af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd321028-d5b4-55f7-9e20-2eaa262f9154</td>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6</td>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e90e721f-00d1-5114-aa5d-5f1061472a29</td>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b215f3a2-d647-59f9-8410-1274ee5edd97</td>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bda61b6-1396-5a2a-a049-0b4035e40d59</td>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "      <td>상거래(쇼핑)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  fd321028-d5b4-55f7-9e20-2eaa262f9154   \n",
       "1  c51be2e4-c8d0-5cea-b1ae-cde1fe8f8ab6   \n",
       "2  e90e721f-00d1-5114-aa5d-5f1061472a29   \n",
       "3  b215f3a2-d647-59f9-8410-1274ee5edd97   \n",
       "4  0bda61b6-1396-5a2a-a049-0b4035e40d59   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary Category  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  상거래(쇼핑)  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  상거래(쇼핑)  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  상거래(쇼핑)  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  상거래(쇼핑)  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  상거래(쇼핑)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84fed279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13994/13994 [00:00<00:00, 33611.65it/s]\n",
      "100%|██████████| 13994/13994 [00:00<00:00, 84410.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_text = []\n",
    "train_summary = []\n",
    "\n",
    "for tt in tqdm(train_df['Text']):\n",
    "    train_text.append(preprocess_sentence(tt))\n",
    "\n",
    "for ts in tqdm(train_df['Summary']):\n",
    "      train_summary.append(preprocess_sentence(ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc282ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1746/1746 [00:00<00:00, 32890.00it/s]\n",
      "100%|██████████| 1746/1746 [00:00<00:00, 84784.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "val_text = []\n",
    "val_summary = []\n",
    "\n",
    "for vt in tqdm(val_df['Text']):\n",
    "    val_text.append(preprocess_sentence(vt))\n",
    "\n",
    "for vs in tqdm(val_df['Summary']):\n",
    "      val_summary.append(preprocess_sentence(vs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4384bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_text,train_summary), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_text,val_summary), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e2a2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...   \n",
       "1  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...   \n",
       "2  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...   \n",
       "3  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...   \n",
       "4  잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3a9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_samples = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c9c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 13994)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 1746)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 1746)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70fcd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07651d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bfe92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87a422dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    input_ids = []\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input))\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "        label_id[i].append(tokenizer.eos_token_id)  \n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_id[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "   \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'decoder_input_ids': dec_input_ids,\n",
    "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f70c2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30022, 768, padding_idx=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_words = [\n",
    "                \"#@주소#\", \"#@이모티콘#\", \"#@이름#\", \"#@URL#\", \"#@소속#\",\n",
    "                \"#@기타#\", \"#@전번#\", \"#@계정#\", \"#@url#\", \"#@번호#\", \"#@금융#\", \"#@신원#\",\n",
    "                \"#@장소#\", \"#@시스템#사진#\", \"#@시스템#동영상#\", \"#@시스템#기타#\", \"#@시스템#검색#\",\n",
    "                \"#@시스템#지도#\", \"#@시스템#삭제#\", \"#@시스템#파일#\", \"#@시스템#송금#\", \"#@시스템#\",\n",
    "                ]\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_words})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87219885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6324c44cbf2436e8d3f98b02b72b57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53961892fd374cd78d4fbc9bbc822116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c151eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 32 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5\n",
    "#model.config.suppress_tokens = [23782, 14338, 22554, 234]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12313f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"/aiffel/aiffel/Korean_Conversation_Summary/checkpoint/total_MLM_test/checkpoint-8500\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30022\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35967504",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a698e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/total_MLM_ft_test\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d78e508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b03b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b34b25cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13994\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n",
      "  Number of trainable parameters = 123876864\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221202_155227-2putj3hr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/2putj3hr\" target=\"_blank\">checkpoint/total_MLM_ft_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4375' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4375/4375 44:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.602200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.711200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-500\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-500/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-1000\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-1000/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-1500\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-1500/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-2000\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-2000/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/total_MLM_ft_test/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-2500\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-2500/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/total_MLM_ft_test/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-3000\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-3000/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/total_MLM_ft_test/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-3500\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-3500/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/total_MLM_ft_test/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/total_MLM_ft_test/checkpoint-4000\n",
      "Configuration saved in checkpoint/total_MLM_ft_test/checkpoint-4000/config.json\n",
      "Model weights saved in checkpoint/total_MLM_ft_test/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/total_MLM_ft_test/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/total_MLM_ft_test/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/total_MLM_ft_test/checkpoint-2500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4375, training_loss=3.1904900669642857, metrics={'train_runtime': 2670.6217, 'train_samples_per_second': 26.2, 'train_steps_per_second': 1.638, 'total_flos': 5332907497881600.0, 'train_loss': 3.1904900669642857, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ced1a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1746\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 02:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'r': 0.23735879214618907, 'p': 0.24028460871470397, 'f': 0.2315765784958787}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'r': 0.09155522382857131, 'p': 0.09355085760848701, 'f': 0.08940263659236039}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'r': 0.2256544253078629, 'p': 0.228327187798837, 'f': 0.22012167721866366}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.4674408435821533,\n",
       " 'eval_rouge-1': {'r': 0.23735879214618907,\n",
       "  'p': 0.24028460871470397,\n",
       "  'f': 0.2315765784958787},\n",
       " 'eval_rouge-2': {'r': 0.09155522382857131,\n",
       "  'p': 0.09355085760848701,\n",
       "  'f': 0.08940263659236039},\n",
       " 'eval_rouge-l': {'r': 0.2256544253078629,\n",
       "  'p': 0.228327187798837,\n",
       "  'f': 0.22012167721866366},\n",
       " 'eval_runtime': 152.363,\n",
       " 'eval_samples_per_second': 11.459,\n",
       " 'eval_steps_per_second': 0.184,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0604a425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /aiffel/aiffel/Korean_Conversation_Summary/checkpoint/total_MLM_test/checkpoint-8500/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"/aiffel/aiffel/Korean_Conversation_Summary/checkpoint/total_MLM_test/checkpoint-8500\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30022\n",
      "}\n",
      "\n",
      "loading weights file /aiffel/aiffel/Korean_Conversation_Summary/checkpoint/total_MLM_test/checkpoint-8500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /aiffel/aiffel/Korean_Conversation_Summary/checkpoint/total_MLM_test/checkpoint-8500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=5,no_repeat_ngram_size=3,\n",
    "                            attention_mask=attention_mask, \n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a971ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1746 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 32 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1746/1746 [06:13<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#summaries_before_tuning = []\n",
    "#for test_sample in tqdm(test_samples):\n",
    "#    summaries_before_tuning.append(generate_summary(test_sample, model_before_tuning)[1])\n",
    "#summaries_before_tuning = list(itertools.chain(*summaries_before_tuning))    \n",
    "    \n",
    "summaries_after_tuning=[]\n",
    "for test_sample in tqdm(test_samples):\n",
    "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n",
    "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab5e8ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.20695432662702204,\n",
       "  'p': 0.23348856688759606,\n",
       "  'f': 0.21181341474640628},\n",
       " 'rouge-2': {'r': 0.07161270246226058,\n",
       "  'p': 0.08234710630354469,\n",
       "  'f': 0.07328339220708147},\n",
       " 'rouge-l': {'r': 0.19625663905658877,\n",
       "  'p': 0.22149201135581725,\n",
       "  'f': 0.200931791403356}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc6280e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "\n",
      "Summary after \n",
      " 영업팀 과장님이 보내줬는데 팀장님이 해줄지 모르겠다며 저번에 부산 갈 때도 숙소로 엄청 싸웠다고 한다\n",
      "\n",
      "Target summary \n",
      " 팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다\n",
      "\n",
      "Text 웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서 웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구 내말이 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고 넘 시러 일단\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_100 \n",
      "\n",
      "Summary after \n",
      " 본사 직원이 한 달에 한 번씩 온다고 해서 면접 보러 온 줄 알았다고 한다\n",
      "\n",
      "Target summary \n",
      " 설빙의 본사 직원이 한 달에 한 번씩 오는데 한 시간 정도 이야기하고 있다\n",
      "\n",
      "Text #@소속#본사직원이왓노 누가 아진짜 뭔 일이래 한달에한번씩 온대 근데 한시간 정도 말하는듯 난 면접보러온줄알앗어 오 몰랐네 무슨얘기 지\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_200 \n",
      "\n",
      "Summary after \n",
      " 이번 주에 비가 오고 장마가 오고 27도까지 올라간다고 한다\n",
      "\n",
      "Target summary \n",
      " 이번 주 날씨가 더웠다가 비 온다는데 별로다\n",
      "\n",
      "Text 근데 이번주 날씨 완전 별로더라 오늘 27도까지 그니까 올라가고 나도 티하나입고왔오 화수목 비오고 장마도 아니고 이게뭐얏 나도 가서 가디건벗을라고 더워 이제 후엉 더워더워 이제 티 한장만 입어야돼 수욜도 비오네 우리영화보는 날인데 오지말라고 기도하자\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_300 \n",
      "\n",
      "Summary after \n",
      " 빵 집 바로 옆에 있는 카페 마가다를 양갱 파는 곳 아니냐고 하자 주소 체육센터 쪽이라고 한다\n",
      "\n",
      "Target summary \n",
      " 주소로 위치를 확인하고 헬스를 두 번 하고 오라 한다\n",
      "\n",
      "Text 아냐 거기야 빵 집 바로옆 아 카페 마가렡 양갱파는곳이 아닌가 저 주소 체육센터쪽이야 이런 가서 헬스 두 번 하고 와 개멀자나\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_400 \n",
      "\n",
      "Summary after \n",
      " 저한테 딱 맞으니 가슴이 좀 클 수도 있으니 얼른 가서 입어보라고 한다\n",
      "\n",
      "Target summary \n",
      " 옷이 클 수도 있어서 얼른 입어봤는데 지퍼 부분이 붕 떠서 싫다고 한다\n",
      "\n",
      "Text #@시스템#사진# 왔따한다 #@이름#온니 큰강 큰가요 저한테딱맞으니 가슴이 좀 클숟 수도 얼른가서 입어볼래 #@이모티콘# 근데 단점 지퍼부분 좀 붕떠 요성 상 시르다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_500 \n",
      "\n",
      "Summary after \n",
      " 저녁에 포항 짐을 풀고 오라방은 맥주를 먹으러 간다\n",
      "\n",
      "Target summary \n",
      " 저녁으로 숙소 근처에서 맛있는 데를 찾아서 대게를 먹을 것이다\n",
      "\n",
      "Text 머양 저녁은 그롬 머먹어 저녁에 대게 먹고시펑 알았쩡 그람 포항 짐풀고 저녁묵장 오라방은 맥쮸한모금만해야딩 저기 말고 딴데가면 되지 딴데도 저런덴 많앙 그냥 숙소 근처에 맛있는데 찾아서 먹자 저기 다음에 가구 알았쯍 알아볼게용\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_600 \n",
      "\n",
      "Summary after \n",
      " 냥이 사료를 주문해달라고 부탁하고 목요일에 도착할 예정이다\n",
      "\n",
      "Target summary \n",
      " 고양이 사료를 얼마나 많이 살지 대화한다\n",
      "\n",
      "Text 냥이 사료 주문해주쎄용 아예 2포하까 60키로 엥 40키로지 #@시스템#사진# 목요일 도착 예정 먼저 간 예삐가 엄마한테 매일매일 고마워할거에요 착한 #@이름# 보내준 엄마 딸도 나도 #@시스템#사진# 엄마는 왜 100 안 나옴 엄마 룰렛은 300부터 시작해요 아니 증말 자랑을 할 수가 없네 내껀 그런가봐 뿡뿡\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_700 \n",
      "\n",
      "Summary after \n",
      " 애교쟁이 마스크가 품절되기 전에 많이 사 놔서 아직은 여유있게 주문할 수 있다\n",
      "\n",
      "Target summary \n",
      " 아직 여유 있게 주문 가능해서 아에르 마스크를 많이 사 놓으려고 한다\n",
      "\n",
      "Text 애교쟁이 마스크는 완료 역시 최고 근데 아에르 있어 응 아직은 여유있게 주문가능해 다행이야 진짜 품절되기전에 많이 사놔야지 #@시스템#사진# 역시 그럼 한달정도는 넉넉하겠다 서방님 응 앞으론 쭉 마스크는 써야될거같아\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_800 \n",
      "\n",
      "Summary after \n",
      " 유명한 웹툰 안나라 수마나라라는 웹툰을 열심히 봤다고 한다\n",
      "\n",
      "Target summary \n",
      " 언니에게 유명한 웹툰 안나라 수마나라가 드라마화된다고 하니 흑백 웹툰이었는데 드라마는 어떻게 연출했을지 기대된다고 한다\n",
      "\n",
      "Text 유명한 웹툰 안나라수마나라 있잖아 어 알지 언니 그거 열심히 봤었어 당시에 그니까 연재하던 당시에 오홍 그렇구나 그럼 언니가 좋아할 수 있는 소식이야 뭔데 그래 안나라수마나라가 드라마화 된대 와 그렇구나 안나라수마나라가 흑백 웹툰이었는데 그만의 분위기를 만들어내는데 한 몫 했다고 생각하거든 드라마는 과연 어떻게 연출했을지 기대되네\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_900 \n",
      "\n",
      "Summary after \n",
      " 코코가 최고 인정인정을 받았는데 학요근처에 학교가 없어서 너무 아쉽다\n",
      "\n",
      "Target summary \n",
      " 코스트코가 최고인데 학교 근처에 없어서 아쉽다\n",
      "\n",
      "Text 역시 코코가 최고 인정인정 학요근처에 코코가없어 학교 앗 너무 아쉽다 그니까하 말두안대\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1000 \n",
      "\n",
      "Summary after \n",
      " 머리카락 염색한 걸 못 봐서 아쉬워했더니 카톡 프로필 사진를 사보라고 한다\n",
      "\n",
      "Target summary \n",
      " 머리 염색한 사진을 카카오톡 프로필 사진으로 확인하라고 한다\n",
      "\n",
      "Text 머리카락 염색한걸 못 봐서 아쉬워했더니 카톡프사보래 쌀알만한크기로나왔으면서 쌀알만한크기 염색 무슨색으로 다크브라운 내가 어두운색이 더 잘어울릴거라고 했거든 오오\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1100 \n",
      "\n",
      "Summary after \n",
      " 전공만 읽어보기만 해도 될 거라서 전공만 알면 된다고 한다\n",
      "\n",
      "Target summary \n",
      " 전공만 알면 되고 다른 것은 읽어 보기만 해도 될 것 같다지만 그래도 한 번은 읽고 가야 할 것 같다\n",
      "\n",
      "Text 전공만 알면되게찌 열공해랑 진짜 다른거는 읽어보기만 해도 될꺼야 라며 위안둥 중 고맙다잉 그랴 일회독은 하고 가여지 적어도\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1200 \n",
      "\n",
      "Summary after \n",
      " 오늘 별로 별로 돌아다니지도 않았는데 이단 같은 사람들이 3번이나 아가씨 하면서 말을 걸었다\n",
      "\n",
      "Target summary \n",
      " 이상한 아저씨가 말을 걸면 안 보이는 척하고 앞만 보고 가야 한다\n",
      "\n",
      "Text 나 오늘 별로 돌아다니지도않았눈데 그 이단같은 사람들있잖아 3번이나 아가씨 하면서말걸고 어떤사람은 방금 하나은행 어딨냐고 이상한 아저씨가 물어봄 너가 말 잘듣게 생겼나바 나도 많이 그럼 얼른지나가 앜 그런가봐 그래서 안보이는척하고 앞만보고갔어 근데 진짜필요함 안하면 피곤해져 맞아 그럼 말안걸엉 대꾸안해줘야됨\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1300 \n",
      "\n",
      "Summary after \n",
      " 15분 정도 가는데 왜 이렇게 똥을 쌌는지 모르겠다\n",
      "\n",
      "Target summary \n",
      " 똥을 싼 사람이 오랫동안 못 쌌다는 사람에게 차전자피를 추천해 준다\n",
      "\n",
      "Text 아 망했다 똥매려 미친 아직 15분정도 가여되는데 아나 똥 쌌니 #@이름# 왜이리 격하게 반응해줘 쌌습니다 똥루 개이덕 시 끄럽게하기 아 똥못싼지 개오래됐누흑흑 엥 그렇다면 차전자피를 추천해줄게\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1400 \n",
      "\n",
      "Summary after \n",
      " 키울울 여지도 안 되고 옆집 꽃순이만 맨날 구경한다\n",
      "\n",
      "Target summary \n",
      " 옆집 꽃순이가 누군지 사진으로 보여주고 수달 사는 것도 아는지 이야기하자 구미가 생태 도시라고 말한다\n",
      "\n",
      "Text 나도 키울 여건도 안되고 옆집 꽃순이만 맨날 구경한다 꽃순이가 누군데 #@시스템#사진# 아까도 봤지 꽃순 야너 금오산 저수지에 수달 사는거 아나 어니 생태도시네 구미\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1500 \n",
      "\n",
      "Summary after \n",
      " 뚱해서 경계하는 눈빛이 귀엽고 가까이가면 쳐서 목욕시키고 싶다\n",
      "\n",
      "Target summary \n",
      " 뚱해서 날카롭게 경계하는 눈빛이 귀엽다고 꼬질꼬질한 모습이 목욕을 시키고 싶다고 한다\n",
      "\n",
      "Text 뭔가 뚱해서 귀엽다 경계오지는 눈빛 긍게 가까이가면 쳐 맞겠다 때꾸정물봐 목욕시키고 싶다 뒤지게맞을듯 눈빛이 아주 날카로와 귀엽다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1600 \n",
      "\n",
      "Summary after \n",
      " 두툼하게 땡김을 먹고 싶은데 참치 아님 연어 회를 먹고 싶다\n",
      "\n",
      "Target summary \n",
      " 참치 회를 김과 같이 먹으면 맛있지만 참치가 안 맞아서 광어와 연어를 먹고 싶다고 한다\n",
      "\n",
      "Text 나 요즘 회 먹고 싶어 어떻게 생각해 긍정적으로 생각해 먹자 흠 두툼하게 땡김 참치 아님 연어 어떰 나 참치가 안 맞아 광어에 연어 각임 참치 맛있는데 김이랑 같이 먹으면 크으 입에서 걍 녹지유\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1700 \n",
      "\n",
      "Summary after \n",
      " 설빙 가서 빙수를 퍼 먹고 싶다고 하자 이번엔 무슨 빙수가 드시고 싶다고 한다\n",
      "\n",
      "Target summary \n",
      " 설빙에 가서 구슬 아이스크림 설빙을 퍼먹고 싶다고 말한다\n",
      "\n",
      "Text 설빙가서 빙수 퍼먹고싶다아 아앙 이번엔 무슨 빙수가 드시고 싶은데 좀 특별해유 특별 구슬아이스크림 설빙 그렇구나 우히히히히\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(summaries_after_tuning), 100):\n",
    "    print('idx_{} '.format(i))\n",
    "    #print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebeb674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
